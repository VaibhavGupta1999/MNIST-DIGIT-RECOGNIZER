{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fe525b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import StringLookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0514ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (128, 32)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "PADDING_TOKEN = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "89622747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters:  ['!', '\"', '#', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "max_len:  19\n"
     ]
    }
   ],
   "source": [
    "DATA_INPUT_PATH = \"F:\\Python\\Projects\\handwrittern\\iam-handwriting-word-database\"\n",
    "\n",
    "images_path = []\n",
    "labels = []\n",
    "\n",
    "def preprocess_dataset():\n",
    "    characters = set()\n",
    "    max_len = 0\n",
    "    with open(os.path.join(DATA_INPUT_PATH, 'iam_words', 'words.txt'), 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        for line_number, line in enumerate(lines):\n",
    "            # Skip comments and empty lines\n",
    "            if line.startswith('#') or line.strip() == '':\n",
    "                continue\n",
    "\n",
    "            # Split the line and extract information\n",
    "            parts = line.strip().split()\n",
    "\n",
    "            # Continue with the rest of the code\n",
    "            word_id = parts[0]\n",
    "\n",
    "            first_folder = word_id.split(\"-\")[0]\n",
    "            second_folder = first_folder + '-' + word_id.split(\"-\")[1]\n",
    "\n",
    "            # Construct the image filename\n",
    "            image_filename = f\"{word_id}.png\"\n",
    "            image_path = os.path.join(DATA_INPUT_PATH, 'iam_words', 'words', first_folder, second_folder, image_filename)\n",
    "\n",
    "            # Check if the image file exists\n",
    "            if os.path.isfile(image_path) and os.path.getsize(image_path):\n",
    "\n",
    "                images_path.append(image_path)\n",
    "\n",
    "                # Extract labels\n",
    "                label = parts[-1].strip()\n",
    "                for char in label:\n",
    "                    characters.add(char)\n",
    "\n",
    "                max_len = max(max_len, len(label))\n",
    "                labels.append(label)\n",
    "\n",
    "    characters = sorted(list(characters))\n",
    "\n",
    "    print('characters: ', characters)\n",
    "    print('max_len: ', max_len)\n",
    "    # Mapping characters to integers.\n",
    "    char_to_num = tf.keras.layers.StringLookup(\n",
    "        vocabulary=list(characters), mask_token=None)\n",
    "\n",
    "    # Mapping integers back to original characters.\n",
    "    num_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)\n",
    "    return characters, char_to_num, num_to_char, max_len\n",
    "    \n",
    "characters, char_to_num, num_to_char, max_len = preprocess_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "32872cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion_free_resize(image, img_size):\n",
    "    w, h = img_size\n",
    "    image = tf.image.resize(image, size=(h, w), preserve_aspect_ratio=True)\n",
    "\n",
    "    # Check tha amount of padding needed to be done.\n",
    "    pad_height = h - tf.shape(image)[0]\n",
    "    pad_width = w - tf.shape(image)[1]\n",
    "\n",
    "    # Only necessary if you want to do same amount of padding on both sides.\n",
    "    if pad_height % 2 != 0:\n",
    "        height = pad_height // 2\n",
    "        pad_height_top = height + 1\n",
    "        pad_height_bottom = height\n",
    "    else:\n",
    "        pad_height_top = pad_height_bottom = pad_height // 2\n",
    "\n",
    "    if pad_width % 2 != 0:\n",
    "        width = pad_width // 2\n",
    "        pad_width_left = width + 1\n",
    "        pad_width_right = width\n",
    "    else:\n",
    "        pad_width_left = pad_width_right = pad_width // 2\n",
    "\n",
    "    image = tf.pad(\n",
    "        image,\n",
    "        paddings=[\n",
    "            [pad_height_top, pad_height_bottom],\n",
    "            [pad_width_left, pad_width_right],\n",
    "            [0, 0],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    image = tf.transpose(image, perm=[1, 0, 2])\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image_path, img_size):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, 1)\n",
    "    image = distortion_free_resize(image, img_size)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "def vectorize_label(label):\n",
    "    label = char_to_num(tf.strings.unicode_split(\n",
    "        label, input_encoding=\"UTF-8\"))\n",
    "    length = tf.shape(label)[0]\n",
    "    pad_amount = max_len - length\n",
    "    label = tf.pad(label, paddings=[[0, pad_amount]],\n",
    "                   constant_values=PADDING_TOKEN)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d812d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_labels(image_path, label):\n",
    "    image = preprocess_image(image_path, IMAGE_SIZE)\n",
    "    label = vectorize_label(label)\n",
    "    return {\"image\": image, \"label\": label}\n",
    "\n",
    "def prepare_dataset(image_paths, labels):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    print('len(image_paths): ', len(image_paths))\n",
    "    print('len(labels): ', len(labels))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels)).map(\n",
    "        process_images_labels, num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    return dataset.batch(BATCH_SIZE).cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a8c9c555",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_img_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [80]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m prepare_dataset(\u001b[43mtrain_img_paths\u001b[49m, train_labels_cleaned)\n\u001b[0;32m      2\u001b[0m validation_ds \u001b[38;5;241m=\u001b[39m prepare_dataset(validation_img_paths, validation_labels_cleaned)\n\u001b[0;32m      3\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m prepare_dataset(test_img_paths, test_labels_cleaned)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_img_paths' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b414f682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(image_paths):  35650\n",
      "len(labels):  35650\n",
      "len(image_paths):  4456\n",
      "len(labels):  4456\n",
      "len(image_paths):  4457\n",
      "len(labels):  4457\n"
     ]
    }
   ],
   "source": [
    "def split_dataset():\n",
    "    # Split the data into training, validation, and test sets using train_test_split\n",
    "    train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "        images_path, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Further split the test set into validation and final test sets\n",
    "    val_images, test_images, val_labels, test_labels = train_test_split(\n",
    "        test_images, test_labels, test_size=0.5, random_state=42\n",
    "    )\n",
    "\n",
    "    train_set = prepare_dataset(train_images, train_labels)\n",
    "    val_set = prepare_dataset(val_images, val_labels)\n",
    "    test_set = prepare_dataset(test_images, test_labels)\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "train_set, val_set, test_set = split_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "26128a95",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_img_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [88]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m prepare_dataset(\u001b[43mtrain_img_paths\u001b[49m, train_labels_cleaned)\n\u001b[0;32m      2\u001b[0m validation_ds \u001b[38;5;241m=\u001b[39m prepare_dataset(validation_img_paths, validation_labels_cleaned)\n\u001b[0;32m      3\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m prepare_dataset(test_img_paths, test_labels_cleaned)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_img_paths' is not defined"
     ]
    }
   ],
   "source": [
    "train_ds = prepare_dataset(train_images, train_labels_cleaned)\n",
    "validation_ds = prepare_dataset(validation_img_paths, validation_labels_cleaned)\n",
    "test_ds = prepare_dataset(test_img_paths, test_labels_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e079da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55ba5279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-ctcmodel in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.0rc3)\n",
      "Requirement already satisfied: keras in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras-ctcmodel) (3.1.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras-ctcmodel) (2.16.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras->keras-ctcmodel) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras->keras-ctcmodel) (1.24.4)\n",
      "Requirement already satisfied: rich in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras->keras-ctcmodel) (13.5.3)\n",
      "Requirement already satisfied: namex in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras->keras-ctcmodel) (0.0.7)\n",
      "Requirement already satisfied: h5py in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras->keras-ctcmodel) (3.10.0)\n",
      "Requirement already satisfied: optree in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras->keras-ctcmodel) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras->keras-ctcmodel) (0.3.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-ctcmodel) (2.16.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (4.24.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (2.28.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (63.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (4.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (2.16.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (0.31.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras->keras-ctcmodel) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras->keras-ctcmodel) (2.16.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (0.43.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-ctcmodel) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (2022.6.15)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (3.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\vaibhav gupta\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow->keras-ctcmodel) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-ctcmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0db863ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = tf.keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * \\\n",
    "            tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * \\\n",
    "            tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # At test time, just return the computed predictions.\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0bf7fd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_17\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_17\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ image[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling2d_22              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling2d_23              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ reshape_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │ reshape_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional_22              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)               │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional_23              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │ bidirectional_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)               │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ label (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,191</span> │ bidirectional_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ ctc_loss (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CTCLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ label[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], dense2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv2d_22 (\u001b[38;5;33mConv2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m320\u001b[0m │ image[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling2d_22              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │ conv2d_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv2d_23 (\u001b[38;5;33mConv2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling2d_23              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │ conv2d_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ reshape_11 (\u001b[38;5;33mReshape\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ max_pooling2d_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m32,832\u001b[0m │ reshape_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │ dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional_22              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m197,632\u001b[0m │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)               │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional_23              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m164,352\u001b[0m │ bidirectional_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)               │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ label (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense2 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m79\u001b[0m)            │          \u001b[38;5;34m10,191\u001b[0m │ bidirectional_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ ctc_loss (\u001b[38;5;33mCTCLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m79\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │ label[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], dense2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">423,823</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m423,823\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">423,823</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m423,823\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_model():\n",
    "    input_img = tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 1), name=\"image\")\n",
    "    labels = tf.keras.layers.Input(name=\"label\", shape=(None,))\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",)(input_img)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(64,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",)(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    new_shape = ((IMAGE_SIZE[0] // 4), (IMAGE_SIZE[1] // 4) * 64)\n",
    "    x = tf.keras.layers.Reshape(target_shape=new_shape)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
    "    x = tf.keras.layers.Dense(len(char_to_num.get_vocabulary()) + 2, activation=\"softmax\", name=\"dense2\")(x)\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
    "    model = tf.keras.models.Model(inputs=[input_img, labels], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "    model.summary()\n",
    "    return model\n",
    "    \n",
    "model = build_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0a097bfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [73]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m validation_images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m validation_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvalidation_ds\u001b[49m:\n\u001b[0;32m      5\u001b[0m     validation_images\u001b[38;5;241m.\u001b[39mappend(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      6\u001b[0m     validation_labels\u001b[38;5;241m.\u001b[39mappend(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'validation_ds' is not defined"
     ]
    }
   ],
   "source": [
    "validation_images = []\n",
    "validation_labels = []\n",
    "\n",
    "for batch in validation_ds:\n",
    "    validation_images.append(batch[\"image\"])\n",
    "    validation_labels.append(batch[\"label\"])\n",
    "def calculate_edit_distance(labels, predictions):\n",
    "    # Get a single batch and convert its labels to sparse tensors.\n",
    "    saprse_labels = tf.cast(tf.sparse.from_dense(labels), dtype=tf.int64)\n",
    "\n",
    "    # Make predictions and convert them to sparse tensors.\n",
    "    input_len = np.ones(predictions.shape[0]) * predictions.shape[1]\n",
    "    predictions_decoded = keras.backend.ctc_decode(\n",
    "        predictions, input_length=input_len, greedy=True\n",
    "    )[0][0][:, :max_len]\n",
    "    sparse_predictions = tf.cast(\n",
    "        tf.sparse.from_dense(predictions_decoded), dtype=tf.int64\n",
    "    )\n",
    "\n",
    "    # Compute individual edit distances and average them out.\n",
    "    edit_distances = tf.edit_distance(\n",
    "        sparse_predictions, saprse_labels, normalize=False\n",
    "    )\n",
    "    return tf.reduce_mean(edit_distances)\n",
    "\n",
    "\n",
    "class EditDistanceCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, pred_model):\n",
    "        super().__init__()\n",
    "        self.prediction_model = pred_model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        edit_distances = []\n",
    "\n",
    "        for i in range(len(validation_images)):\n",
    "            labels = validation_labels[i]\n",
    "            predictions = self.prediction_model.predict(validation_images[i])\n",
    "            edit_distances.append(calculate_edit_distance(labels, predictions).numpy())\n",
    "\n",
    "        print(\n",
    "            f\"Mean edit distance for epoch {epoch + 1}: {np.mean(edit_distances):.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e50a24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EditDistanceCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, pred_model, max_len, validation_images, validation_labels):\n",
    "        super().__init__()\n",
    "        self.prediction_model = pred_model\n",
    "        self.max_len = max_len\n",
    "        self.validation_images = validation_images\n",
    "        self.validation_labels = validation_labels\n",
    "\n",
    "    def calculate_edit_distance(self, labels, predictions, max_len):\n",
    "        # Get a single batch and convert its labels to sparse tensors.\n",
    "        saprse_labels = tf.cast(tf.sparse.from_dense(labels), dtype=tf.int64)\n",
    "\n",
    "        # Make predictions and convert them to sparse tensors.\n",
    "        input_len = np.ones(predictions.shape[0]) * predictions.shape[1]\n",
    "        predictions_decoded = tf.keras.backend.ctc_decode(\n",
    "            predictions, input_length=input_len, greedy=True\n",
    "        )[0][0][:, :max_len]\n",
    "        sparse_predictions = tf.cast(\n",
    "            tf.sparse.from_dense(predictions_decoded), dtype=tf.int64\n",
    "        )\n",
    "\n",
    "        # Compute individual edit distances and average them out.\n",
    "        edit_distances = tf.edit_distance(\n",
    "            sparse_predictions, saprse_labels, normalize=False\n",
    "        )\n",
    "        return tf.reduce_mean(edit_distances)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        edit_distances = []\n",
    "\n",
    "        for i in range(len(self.validation_images)):\n",
    "            labels = self.validation_labels[i]\n",
    "            predictions = self.prediction_model.predict(\n",
    "                self.validation_images[i])\n",
    "            edit_distances.append(self.calculate_edit_distance(\n",
    "                labels, predictions, self.max_len).numpy())\n",
    "\n",
    "        print(\n",
    "            f\"Mean edit distance for epoch {epoch + 1}: {np.mean(edit_distances):.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dd62df1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [90]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m      5\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39mearly_stopping_patience, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mtrain_dataset\u001b[49m,\n\u001b[0;32m     11\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mvalidation_dataset,\n\u001b[0;32m     12\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m     13\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping],)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "early_stopping_patience = 10\n",
    "# Add early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20c40afa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`inputs` argument cannot be empty. Received:\ninputs=[]\noutputs=<KerasTensor shape=(None, 32, 79), dtype=float32, sparse=False, name=keras_tensor_112>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     hist \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     16\u001b[0m         train_set,\n\u001b[0;32m     17\u001b[0m         validation_data\u001b[38;5;241m=\u001b[39mval_set,\n\u001b[0;32m     18\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[0;32m     19\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[edit_distance_callback, early_stopping],\n\u001b[0;32m     20\u001b[0m     )\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hist, prediction_model\n\u001b[1;32m---> 23\u001b[0m history, prediction_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     validation_images\u001b[38;5;241m.\u001b[39mappend(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      9\u001b[0m     validation_labels\u001b[38;5;241m.\u001b[39mappend(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 11\u001b[0m prediction_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdense2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m edit_distance_callback \u001b[38;5;241m=\u001b[39m EditDistanceCallback(prediction_model, max_len, validation_images, validation_labels)\n\u001b[0;32m     13\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\models\\model.py:143\u001b[0m, in \u001b[0;36mModel.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m functional_init_arguments(args, kwargs) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m==\u001b[39m Model:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functional\u001b[38;5;241m.\u001b[39mFunctional(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tracking.py:28\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m DotNotTrackScope():\n\u001b[1;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\models\\functional.py:157\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[1;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m([is_input_keras_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mflatten(inputs)]):\n\u001b[0;32m    155\u001b[0m     inputs, outputs \u001b[38;5;241m=\u001b[39m clone_graph_nodes(inputs, outputs)\n\u001b[1;32m--> 157\u001b[0m Function\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, outputs, name\u001b[38;5;241m=\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m trainable\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\ops\\function.py:62\u001b[0m, in \u001b[0;36mFunction.__init__\u001b[1;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(outputs)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs:\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`inputs` argument cannot be empty. Received:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m     )\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`outputs` argument cannot be empty. Received:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: `inputs` argument cannot be empty. Received:\ninputs=[]\noutputs=<KerasTensor shape=(None, 32, 79), dtype=float32, sparse=False, name=keras_tensor_112>"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "\n",
    "    validation_images = []\n",
    "    validation_labels = []\n",
    "    \n",
    "\n",
    "    for batch in val_set:\n",
    "        validation_images.append(batch[\"image\"])\n",
    "        validation_labels.append(batch[\"label\"])\n",
    "\n",
    "    prediction_model = tf.keras.models.Model(model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output)\n",
    "    edit_distance_callback = EditDistanceCallback(prediction_model, max_len, validation_images, validation_labels)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "    # Train the model.\n",
    "    hist = model.fit(\n",
    "        train_set,\n",
    "        validation_data=val_set,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[edit_distance_callback, early_stopping],\n",
    "    )\n",
    "    return hist, prediction_model\n",
    "\n",
    "history, prediction_model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f355c006",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[0;32m     14\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 16\u001b[0m visualize_train_history(\u001b[43mhistory\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "def visualize_train_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_train_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e22aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    accuracy = model.evaluate(test_set)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "evaluate_model() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da6c0882",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(MODEL_OUTPUT_PATH, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m     prediction_model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m     11\u001b[0m         MODEL_OUTPUT_PATH, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 13\u001b[0m \u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mSave the trained HTR model.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(MODEL_OUTPUT_PATH, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mprediction_model\u001b[49m\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m     11\u001b[0m     MODEL_OUTPUT_PATH, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction_model' is not defined"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'MODEL_NAME'\n",
    "MODEL_OUTPUT_PATH = '/kaggle/working/'\n",
    "        \n",
    "def save_model():\n",
    "    \"\"\"\n",
    "    Save the trained HTR model.\n",
    "    \n",
    "    \"\"\"\n",
    "    os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "    prediction_model.save(os.path.join(\n",
    "        MODEL_OUTPUT_PATH, f'{MODEL_NAME}.keras'))\n",
    "    \n",
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec71cfef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m batch_labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     21\u001b[0m _, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m---> 23\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_model\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(batch_images)\n\u001b[0;32m     24\u001b[0m pred_texts \u001b[38;5;241m=\u001b[39m decode_batch_predictions(preds)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m16\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction_model' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search.\n",
    "    results = tf.keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n",
    "        :, :max_len\n",
    "    ]\n",
    "\n",
    "    # Iterate over the results and get back the text.\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n",
    "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "#  Let's check results on some test samples.\n",
    "for batch in test_set.take(2):\n",
    "    batch_images = batch[\"image\"]\n",
    "    batch_labels = batch[\"label\"]\n",
    "    _, ax = plt.subplots(4, 4, figsize=(15, 8))\n",
    "\n",
    "    preds = prediction_model.predict(batch_images)\n",
    "    pred_texts = decode_batch_predictions(preds)\n",
    "\n",
    "    for i in range(16):\n",
    "        img = batch_images[i]\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        img = tf.transpose(img, perm=[1, 0, 2])\n",
    "        img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n",
    "        img = img[:, :, 0]\n",
    "\n",
    "        title = f\"Prediction: {pred_texts[i]}\"\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(title)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0151478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
